{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import _Merge\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "import cv2  # for image processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "import os\n",
    "import h5py\n",
    "from arts_preprocess_utils import load_dataset\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from IPython import display\n",
    "from wassertstein_utils import RandomWeightedAverage, gradient_penalty_loss, wasserstein_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "TRAINING_RATIO = 5  # The training ratio is the number of discriminator updates per generator update. The paper uses 5.\n",
    "GRADIENT_PENALTY_WEIGHT = 10  # As per the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! remember to clear session/graph if you rebuild your graph to avoid out-of-memory errors !!!\n",
    "def reset_tf_session():\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    s = K.get_session()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig = load_dataset('/root/work/datasets/wikiart_mini_portrait.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_Height = train_set_x_orig.shape[1]\n",
    "img_Width = train_set_x_orig.shape[2]\n",
    "N_CLASSES = len(np.unique(test_set_y_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set_x_orig\n",
    "y_train = train_set_y_orig\n",
    "\n",
    "X_dev = test_set_x_orig\n",
    "y_dev = test_set_y_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0][...,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and generator base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Discriminator_model, Generator_model_complex\n",
    "\n",
    "code_shape = 100\n",
    "\n",
    "#based on art-DCGAN (robbiebarrat)\n",
    "generator_model = Generator_model_complex(filters=80, code_shape= (1,1,code_shape))\n",
    "generator = generator_model.get_model((img_Height, img_Width, 3))\n",
    "\n",
    "#based on art-DCGAN (robbiebarrat)\n",
    "discriminator_model = Discriminator_model(filters=40, code_shape=code_shape, include_top = False)\n",
    "discriminator = discriminator_model.get_model((img_Height, img_Width, 3), N_CLASSES, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as L\n",
    "\n",
    "#add top\n",
    "discriminator.add(L.Flatten())\n",
    "discriminator.add(L.Dense(1, kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creater generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#Define graph for generator\n",
    "\n",
    "#discriminator.trainable = False\n",
    "#generator.trainable = True\n",
    "\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in generator.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "generator_input = L.Input(shape=(1,1,code_shape))\n",
    "generator_layers = generator(generator_input)\n",
    "discriminator_layers_for_generator = discriminator(generator_layers)\n",
    "generator_model = Model(inputs=[generator_input], outputs=[discriminator_layers_for_generator])\n",
    "# We use the Adam paramaters from Gulrajani et al.\n",
    "generator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9), loss=wasserstein_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "#Define graph for discriminator\n",
    "\n",
    "#discriminator.trainable = True\n",
    "#generator.trainable = False\n",
    "\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in generator.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "real_samples = L.Input(shape=X_train.shape[1:])\n",
    "generator_input_for_discriminator = L.Input(shape=(1,1,code_shape))\n",
    "generated_samples_for_discriminator = generator(generator_input_for_discriminator)\n",
    "discriminator_output_from_generator = discriminator(generated_samples_for_discriminator)\n",
    "discriminator_output_from_real_samples = discriminator(real_samples)\n",
    "averaged_samples = RandomWeightedAverage(BATCH_SIZE)([real_samples, generated_samples_for_discriminator])\n",
    "averaged_samples_out = discriminator(averaged_samples)\n",
    "\n",
    "discriminator_model = Model(inputs=[real_samples, generator_input_for_discriminator],\n",
    "                            outputs=[discriminator_output_from_real_samples,\n",
    "                                     discriminator_output_from_generator,\n",
    "                                     averaged_samples_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define loss fucntions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# The gradient penalty loss function requires the input averaged samples to get gradients. However,\n",
    "# Keras loss functions can only have two arguments, y_true and y_pred. We get around this by making a partial()\n",
    "# of the function with the averaged samples here.\n",
    "partial_gp_loss = partial(gradient_penalty_loss,\n",
    "                          averaged_samples=averaged_samples,\n",
    "                          gradient_penalty_weight=10)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty'  # Functions need names or Keras will throw an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\n",
    "                            loss=[wasserstein_loss,\n",
    "                                  wasserstein_loss,\n",
    "                                  partial_gp_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_positive = np.ones_like(y_train)\n",
    "y_train_positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_y = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n",
    "negative_y = -positive_y\n",
    "dummy_y = np.zeros((BATCH_SIZE, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator( \n",
    "    rescale = 1.0/255.,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "validation_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "validation_datagen.fit(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan_utils import noisy_images, sample_images, sample_probas\n",
    "\n",
    "def train_gan(X, y_train, datagen, gen_size, epochs = 5, sample_interval = 1000):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        d_loss_hist = []\n",
    "        g_loss_hist = []\n",
    "        size = (BATCH_SIZE,) + gen_size\n",
    "\n",
    "        minibatches_size = BATCH_SIZE * TRAINING_RATIO\n",
    "        batches = 0\n",
    "        \n",
    "        for x_batch, y_batch in datagen.flow(X, y_train, batch_size=minibatches_size):\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            print(x_batch.shape)\n",
    "            for j in range(TRAINING_RATIO):\n",
    "                image_batch = x_batch[j * BATCH_SIZE:(j + 1) * BATCH_SIZE]\n",
    "                noise = np.random.normal(0, 1, size=size)\n",
    "                d_loss =discriminator_model.train_on_batch([image_batch, noise], [positive_y, negative_y, dummy_y])\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, size=size)\n",
    "\n",
    "            g_loss = generator_model.train_on_batch( noise, positive_y)\n",
    "            \n",
    "            if batches >= len(X) / minibatches_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "                break\n",
    "\n",
    "        # Plot the progress \n",
    "        display.clear_output(wait=True)\n",
    "        print (\"%d [D loss: %f] [D(G(z)) loss: %f] loss: %f\" % (epoch, d_loss[1], d_loss[2], d_loss[0]))\n",
    "        g_loss_hist.append(g_loss)\n",
    "        d_loss_hist.append(d_loss[0])\n",
    "        sample_size = (1000,) + gen_size\n",
    "        #TODO:change because discriminator do not classify between 0-1\n",
    "        sample_probas(X, 1000, sample_size, discriminator=discriminator, generator=generator)\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch, gen_size, generator)\n",
    "            #checkpoint to save weights\n",
    "            generator.save_weights(filepath='generator_wasserstein.h5')\n",
    "            discriminator.save_weights(filepath='discriminator_wasserstein.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(X=X_train, y_train=y_train_positive, datagen=train_datagen, gen_size=(1,1,100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

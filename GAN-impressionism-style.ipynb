{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "import cv2  # for image processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "import os\n",
    "import h5py\n",
    "from arts_preprocess_utils import load_dataset\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! remember to clear session/graph if you rebuild your graph to avoid out-of-memory errors !!!\n",
    "def reset_tf_session():\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    s = K.get_session()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig = load_dataset('./wikiart_mini_portrait.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_Height = train_set_x_orig.shape[1]\n",
    "img_Width = train_set_x_orig.shape[2]\n",
    "N_CLASSES = len(np.unique(test_set_y_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set_x_orig\n",
    "y_train = train_set_y_orig\n",
    "\n",
    "X_dev = test_set_x_orig\n",
    "y_dev = test_set_y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get impressionist images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imp_index = np.argwhere(y_train == 1).reshape((-1,))\n",
    "test_imp_index = np.argwhere(y_dev == 1 ).reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp = X_train[train_imp_index, :, :, :]\n",
    "X_dev_imp = X_dev[test_imp_index,:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imp = np.concatenate((X_train, X_dev), axis=0)\n",
    "X_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize images\n",
    "X_imp = X_imp * (1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_imp[0][...,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Discriminator_model\n",
    "\n",
    "#based on art-DCGAN (robbiebarrat)\n",
    "discriminator_model = Discriminator_model(filters=40, code_shape=100)\n",
    "discriminator = discriminator_model.get_model((img_Height, img_Width, 3), N_CLASSES, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.load_weights('./discriminator01.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complex generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Generator_model_complex\n",
    "\n",
    "#based on art-DCGAN (robbiebarrat)\n",
    "generator_model = Generator_model_complex(filters=80, code_shape= (1,1,NOISE), leaky_alpha= 0.001)\n",
    "generator = generator_model.get_model((img_Height, img_Width, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights from a pretrained autoencoder\n",
    "generator.load_weights('./decoder01.h5')\n",
    "#load pre-trained weights\n",
    "#generator.load_weights('./generator01.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discriminator model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in generator.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "discriminator.trainable = True\n",
    "generator.trainable = False\n",
    "\n",
    "real_samples = L.Input(shape=X_imp.shape[1:], name='real_samples')\n",
    "noisy_input = L.Input(shape=(1,1, NOISE))\n",
    "\n",
    "generated_samples = generator(noisy_input)\n",
    "generated_samples_prediction = discriminator(generated_samples)\n",
    "real_samples_prediction = discriminator(real_samples)\n",
    "\n",
    "discriminator_model = Model(inputs=[real_samples,noisy_input], \n",
    "                            outputs=[real_samples_prediction, generated_samples_prediction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model.compile(\n",
    "    loss='binary_crossentropy',  \n",
    "    loss_weights=[0.5, 0.5],\n",
    "    optimizer=keras.optimizers.adamax(lr=1e-3),\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generator model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in generator.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "discriminator.trainable = False\n",
    "generator.trainable = True\n",
    "\n",
    "z = L.Input(shape=(1,1, NOISE))\n",
    "img = generator(z)\n",
    "\n",
    "real = discriminator(img)\n",
    "generator_model = Model(z, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model.compile(\n",
    "    loss='binary_crossentropy',  \n",
    "    optimizer=keras.optimizers.adamax(lr=1e-4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = np.ones((batch_size,))\n",
    "fake = np.zeros((batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan_utils import noisy_images, sample_images, sample_probas\n",
    "\n",
    "def train_gan(X, gen_size, epochs = 200000, sample_interval = 5000):\n",
    "    \n",
    "    #TODO add noise to real images after 2000 epochs\n",
    "    \n",
    "    g_loss_hist = []\n",
    "    d_loss_hist = []\n",
    "    size = (batch_size,) + gen_size\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "        imgs = X[idx]\n",
    "        if (epoch % 10) == 0:\n",
    "            n = int(batch_size / 2)\n",
    "            noisy_imgs = noisy_images(imgs[:n])\n",
    "            imgs = np.concatenate([imgs[n:,], noisy_imgs])\n",
    "\n",
    "        #Generate noise for generator\n",
    "        noise = np.random.normal(0, 1, size=size)\n",
    "        \n",
    "        #train output = [general loss, loss D(x), loss D(G(z)), acc D(x), acc D(G(z)) ]\n",
    "        d_loss = discriminator_model.train_on_batch([imgs, noise], [valid,fake])\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        #Generate noise for generator\n",
    "        noise = np.random.normal(0, 1,  size=size)\n",
    "\n",
    "        # Train the generator (to have the discriminator label samples as valid)\n",
    "        g_loss = generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "        # Plot the progress each 100 epoch\n",
    "        if (epoch % 100) == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] loss: %f\" % (epoch, d_loss[1], 100*d_loss[3], d_loss[2], d_loss[0]))\n",
    "            g_loss_hist.append(g_loss)\n",
    "            d_loss_hist.append(d_loss[0])\n",
    "            sample_size = (1000,) + gen_size\n",
    "            sample_probas(X, 1000, sample_size, discriminator=discriminator, generator=generator)\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch, gen_size, generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(X_imp, gen_size=(1,1,NOISE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save_weights(filepath='generator01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.save_weights(filepath='discriminator01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_imp * (1./255)\n",
    "valid = np.ones((X.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = discriminator.predict(X).round().reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test on a new image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('berni_retrato.jpg')\n",
    "img = cv2.resize(img, (img_Height, img_Width), interpolation=cv2.INTER_CUBIC)\n",
    "img_norm = img *(1./255)\n",
    "img_norm = np.expand_dims(img_norm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_norm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.predict(img_norm).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.uniform(0, 1, size=[100, 1, 1, 100])\n",
    "fakes = generator.predict(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fakes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakes[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

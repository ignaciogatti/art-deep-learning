{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "import cv2  # for image processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "import os\n",
    "import h5py\n",
    "from arts_preprocess_utils import load_dataset\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! remember to clear session/graph if you rebuild your graph to avoid out-of-memory errors !!!\n",
    "def reset_tf_session():\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    s = K.get_session()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig = load_dataset('./wikiart_mini_portrait.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_Height = train_set_x_orig.shape[1]\n",
    "img_Width = train_set_x_orig.shape[2]\n",
    "N_CLASSES = len(np.unique(test_set_y_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set_x_orig\n",
    "y_train = train_set_y_orig\n",
    "\n",
    "X_dev = test_set_x_orig\n",
    "y_dev = test_set_y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get impressionist images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imp_index = np.argwhere(y_train == 1).reshape((-1,))\n",
    "test_imp_index = np.argwhere(y_dev == 1 ).reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp = X_train[train_imp_index, :, :, :]\n",
    "X_dev_imp = X_dev[test_imp_index,:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imp = np.concatenate((X_train, X_dev), axis=0)\n",
    "X_imp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.clip(X_imp[0], 0, 255).astype('uint8')[...,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Inception_model\n",
    "\n",
    "inception_model = Inception_model()\n",
    "model = inception_model.get_model((img_Height, img_Width, 3), N_CLASSES, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./inception01.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ResNet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ResNet_model\n",
    "\n",
    "resnet_model = ResNet_model()\n",
    "model = resNet_model.get_model((img_Height, img_Width, 3), N_CLASSES, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Discriminator_model\n",
    "\n",
    "#based on art-DCGAN (robbiebarrat)\n",
    "#change LeakyReLU parameter\n",
    "discriminator_model = Discriminator_model(filters=40, code_shape=100)\n",
    "model = discriminator_model.get_model((img_Height, img_Width, 3), N_CLASSES, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all layers trainable by default and prepare batch norm for fine-tuning\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = True\n",
    "    if isinstance(layer, keras.layers.BatchNormalization):\n",
    "        # we do aggressive exponential smoothing of batch norm \n",
    "        # parameters to faster adjust to our new dataset\n",
    "        layer.momentum = 0.8\n",
    "    \n",
    "# fix deep layers (fine-tuning only last n)\n",
    "for layer in discriminator.layers[:-5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.load_weights('./discriminator01.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_imp * (1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = np.ones((X.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.predict(x=X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.train_on_batch(x=X[:10], y=valid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = discriminator.predict(X).round().reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('berni_retrato.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_norm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.resize(img, (img_Height, img_Width), interpolation=cv2.INTER_CUBIC)\n",
    "img_norm = img *(1./255)\n",
    "img_norm = np.expand_dims(img_norm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.predict(img_norm).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Generator_model\n",
    "\n",
    "generator_model = Generator_model()\n",
    "generator = generator_model.get_model(NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complex generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Generator_model_complex\n",
    "\n",
    "#based on art-DCGAN (robbiebarrat)\n",
    "generator_model = Generator_model_complex(filters=80, code_shape= (1,1,NOISE))\n",
    "generator = generator_model.get_model((img_Height, img_Width, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights from a pretrained autoencoder\n",
    "generator.load_weights('./decoder01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_weights('./generator01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.compile(loss='binary_crossentropy',\n",
    "                 optimizer=keras.optimizers.adamax(lr=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.uniform(0, 1, size=[100, 1, 1, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakes = generator.predict(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fakes[90][...,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.predict(fakes).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliar function to save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, gen_size):\n",
    "    r, c = 2, 2\n",
    "    size = (r*c,) + gen_size\n",
    "    noise = np.random.normal(0, 1, size= size)\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/%d.png\" % epoch)\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auxiliar function to see probas distribution D(x) and D(G(z))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_probas(batch_size, gen_size):\n",
    "    plt.title('Generated vs real data')\n",
    "    \n",
    "    # Select a random batch of images\n",
    "    idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "    imgs = X[idx]\n",
    "    plt.hist(discriminator.predict(imgs)[:,0],\n",
    "             label='D(x)', alpha=0.5,range=[0,1])\n",
    "    \n",
    "    #Generate random input\n",
    "    noise = np.random.normal(0, 1, size=gen_size)\n",
    "    plt.hist(discriminator.predict(generator.predict(noise))[:,0],\n",
    "             label='D(G(z))',alpha=0.5,range=[0,1])\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',  \n",
    "    optimizer=keras.optimizers.adamax(lr=1e-4),\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "\n",
    "z = L.Input(shape=(1,1, NOISE))\n",
    "img = generator(z)\n",
    "\n",
    "discriminator.trainable = False\n",
    "real = discriminator(img)\n",
    "combined_model = Model(z, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.compile(\n",
    "    loss='binary_crossentropy',  \n",
    "    optimizer=keras.optimizers.adamax(lr=1e-4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Rescale 0 to 1\n",
    "X = X_imp * (1./255)\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = np.ones((batch_size,))\n",
    "fake = np.zeros((batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gen_size, epochs = 10000, sample_interval = 2000):\n",
    "    \n",
    "    g_loss_hist = []\n",
    "    d_loss_hist = []\n",
    "    size = (batch_size,) + gen_size\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "        imgs = X[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, size=size)\n",
    "\n",
    "        # Generate a batch of new images\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        noise = np.random.normal(0, 1,  size=size)\n",
    "\n",
    "        # Train the generator (to have the discriminator label samples as valid)\n",
    "        g_loss = combined_model.train_on_batch(noise, valid)\n",
    "\n",
    "        # Plot the progress each 100 epoch\n",
    "        if (epoch % 100) == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            g_loss_hist.append(g_loss)\n",
    "            d_loss_hist.append(d_loss[0])\n",
    "            sample_size = (1000,) + gen_size\n",
    "            sample_probas(1000, sample_size)\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch, gen_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(gen_size=(1,1,NOISE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save_weights(filepath='generator01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.save_weights(filepath='discriminator01.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
